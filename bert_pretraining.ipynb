{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_pretraining",
      "provenance": [],
      "mount_file_id": "1-KMfcCYjFoOfB83KEZItLahrAi5eLUJX",
      "authorship_tag": "ABX9TyM4zhdrNSrCQOQZzC/atS2o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabhd786/bert_pytorch/blob/master/bert_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beMvNuOqvmLz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "c3a87529-3601-4343-9bab-4843c18a6552"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.13.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.16.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.13->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv1Y10C8cBhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from random import randint, shuffle\n",
        "from random import random as rand\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu5PNtgOvjHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcVSnItOg6ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seek_random_offset(f, back_margin=2000):\n",
        "    \"\"\" seek random offset of file pointer \"\"\"\n",
        "    f.seek(0, 2)\n",
        "    max_offset = f.tell() - back_margin\n",
        "    f.seek(randint(0, max_offset), 0)\n",
        "    f.readline() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEwQoiMsjVW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader():\n",
        "    \"\"\" Load sentence pair from corpus \"\"\"\n",
        "    def __init__(self, file, batch_size, max_len, short_sampling_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.f_pos = open(file, \"r\", encoding='utf-8', errors='ignore')\n",
        "        self.f_neg = open(file, \"r\", encoding='utf-8', errors='ignore') \n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.max_len = max_len \n",
        "        self.short_sampling_prob = short_sampling_prob\n",
        "        self.batch_size = batch_size\n",
        "        self.preproc= Preprocess4Pretrain(max_len*0.15,0.15)\n",
        "\n",
        "    def read_tokens(self, f, length, discard_last_and_restart=True):\n",
        "        \"\"\" Read tokens from file pointer with limited length \"\"\"\n",
        "        tokens = []\n",
        "        while len(tokens) < length:\n",
        "            line = f.readline()\n",
        "            if not line: # end of file\n",
        "                return None\n",
        "            if not line.strip(): \n",
        "                if discard_last_and_restart:\n",
        "                    continue\n",
        "                else:\n",
        "                    return tokens \n",
        "            tokens.extend(self.tokenizer.tokenize(line.strip()))\n",
        "            \n",
        "        return tokens\n",
        "\n",
        "    def __iter__(self): # iterator to load data\n",
        "        while True:\n",
        "            batch = []\n",
        "            for i in range(self.batch_size):\n",
        "             \n",
        "                len_tokens = randint(1, int(self.max_len / 2)) \\\n",
        "                    if rand() < self.short_sampling_prob \\\n",
        "                    else int(self.max_len / 2)\n",
        "\n",
        "                is_next = rand() < 0.5 # whether token_b is next to token_a or not\n",
        "\n",
        "                tokens_a = self.read_tokens(self.f_pos, len_tokens, True)\n",
        "                seek_random_offset(self.f_neg)\n",
        "                f_next = self.f_pos if is_next else self.f_neg\n",
        "                tokens_b = self.read_tokens(f_next, len_tokens, False)\n",
        "\n",
        "                if tokens_a is None or tokens_b is None: \n",
        "                    self.f_pos.seek(0, 0)\n",
        "                    return\n",
        "\n",
        "                data = (is_next, tokens_a, tokens_b)\n",
        "                data=self.preproc(data)\n",
        "                \n",
        "                batch.append(instance)\n",
        "\n",
        "            batch_tensors = [torch.tensor(x, dtype=torch.long) for x in zip(*batch)]\n",
        "            yield batch_tensors\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jzclsinxI0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_loader=DataLoader(\"/content/drive/My Drive/bert_data.txt\",2,512)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHXwFa1q0Wbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def truncate_tokens_pair(tokens_a, tokens_b, max_len):\n",
        "    while True:\n",
        "        if len(tokens_a) + len(tokens_b) <= max_len:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "def get_random_word(vocab_words):\n",
        "    i = random.randint(0,30000)\n",
        "    return list(vocab_words)[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPVWvXoR9iVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocess():\n",
        "    \"\"\" Pre-processing steps for pretraining transformer \"\"\"\n",
        "    def __init__(self, max_pred, mask_prob, max_len=512):\n",
        "        super().__init__()\n",
        "        self.max_pred = max_pred \n",
        "        self.mask_prob = mask_prob \n",
        "        self.indexer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __call__(self,data):\n",
        "        is_next, tokens_a, tokens_b = data\n",
        "        truncate_tokens_pair(tokens_a, tokens_b, self.max_len - 3)\n",
        "\n",
        "        # Add Special Tokens\n",
        "        tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
        "        segment_ids = [0]*(len(tokens_a)+2) + [1]*(len(tokens_b)+1)\n",
        "        input_mask = [1]*len(tokens)\n",
        "\n",
        "        # For masked Language Models\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        n_pred = min(self.max_pred, max(1, int(round(len(tokens)*self.mask_prob))))\n",
        "        cand_pos = [i for i, token in enumerate(tokens)\n",
        "                    if token != '[CLS]' and token != '[SEP]']\n",
        "        shuffle(cand_pos)\n",
        "        for pos in cand_pos[:int(n_pred)]:\n",
        "            masked_tokens.append(tokens[pos])\n",
        "            masked_pos.append(pos)\n",
        "            if rand() < 0.8: # 80%\n",
        "                tokens[pos] = '[MASK]'\n",
        "            elif rand() < 0.5: # 10%\n",
        "                tokens[pos] = get_random_word(self.indexer.vocab)\n",
        "        masked_weights = [1]*len(masked_tokens)\n",
        "\n",
        "        # Token Indexing\n",
        "        input_ids = self.indexer.convert_tokens_to_ids(tokens)\n",
        "        masked_ids = self.indexer.convert_tokens_to_ids(masked_tokens)\n",
        "\n",
        "        # Zero Padding\n",
        "        n_pad = self.max_len - len(input_ids)\n",
        "        input_ids.extend([0]*int(n_pad))\n",
        "        segment_ids.extend([0]*int(n_pad))\n",
        "        input_mask.extend([0]*int(n_pad))\n",
        "\n",
        "        # Zero Padding for masked target\n",
        "        if self.max_pred > n_pred:\n",
        "            n_pad = self.max_pred - n_pred\n",
        "            masked_ids.extend([0]*int(n_pad))\n",
        "            masked_pos.extend([0]*int(n_pad))\n",
        "            masked_weights.extend([0]*int(n_pad))\n",
        "\n",
        "        return (input_ids, segment_ids, input_mask, masked_ids, masked_pos, masked_weights, is_next)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv_A32e_7HSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gelu(x):\n",
        "    \"Implementation of the gelu activation function by Hugging Face\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim, variance_epsilon=1e-12):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(dim))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW50SS28rveE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class embedding(nn.Module):\n",
        "  def __init__(self,dim,vocab_size,max_len,n_segs):\n",
        "    super().__init__()\n",
        "    self.embed=nn.Embedding(vocab_size,dim)\n",
        "    self.embedpos=nn.Embedding(max_len,dim)\n",
        "    self.segembed=nn.Embedding(n_segs,dim)\n",
        "    self.norm = LayerNorm(dim)\n",
        "    self.drop = nn.Dropout(0.1)\n",
        "  def forward(self,x,seg):\n",
        "    seq_len = x.size(1)\n",
        "    pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
        "    pos = pos.unsqueeze(0).expand_as(x)\n",
        "    return self.norm(self.drop(self.embed(x)+self.embedpos(pos)+self.segembed(seg)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBu84yBA45MA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self,dim,heads,max_len):\n",
        "    super().__init__()\n",
        "    self.q_mat=nn.Linear(dim,dim)\n",
        "    self.k_mat=nn.Linear(dim,dim)\n",
        "    self.v_mat=nn.Linear(dim,dim)\n",
        "    self.dim=dim\n",
        "    self.heads=heads\n",
        "    self.max_len=max_len\n",
        "    self.dk=dim//heads\n",
        "    self.drop=nn.Dropout(0.1)\n",
        "    self.softmax=nn.Softmax(-1)\n",
        "    self.out = nn.Linear(dim,dim)\n",
        "  def forward(self,x,mask=None):\n",
        "    bs=x.size(0)\n",
        "    q=self.q_mat(x).view(bs,-1,self.heads,self.dk)\n",
        "    k=self.k_mat(x).view(bs,-1,self.heads,self.dk)\n",
        "    v=self.v_mat(x).view(bs,-1,self.heads,self.dk)\n",
        "\n",
        "    q=q.transpose(1,2)\n",
        "    k=k.transpose(1,2)\n",
        "    v=v.transpose(1,2)\n",
        "\n",
        "    scores=torch.matmul(q,k.transpose(2,3))/math.sqrt(self.dk)\n",
        "\n",
        "    if mask is not None:\n",
        "            mask = mask[:, None, None, :].float()\n",
        "            scores -= 10000.0 * (1.0 - mask)\n",
        "\n",
        "    scores = self.drop(self.softmax(scores))\n",
        "    output = torch.matmul(scores, v)\n",
        "\n",
        "    concat = output.transpose(1,2).contiguous()\\\n",
        "    .view(bs, -1, self.dim)\n",
        "\n",
        "    output=self.out(concat)   \n",
        "  \n",
        "    \n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjjr7ETh4-6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class feedforward(nn.Module):\n",
        "  def __init__(self,dim,heads,max_len):\n",
        "    super().__init__()\n",
        "    self.fc1=nn.Linear(dim,dim*4)\n",
        "    self.fc2=nn.Linear(dim*4,dim)\n",
        "  def forward(self,x):\n",
        "    out=self.fc2(gelu(self.fc1(x)))\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJPv4x-bN8f3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,dim,heads,max_len):\n",
        "    super().__init__()\n",
        "    self.attention=Attention(dim,heads,max_len)\n",
        "    self.norm1=LayerNorm(dim)\n",
        "    self.ff=feedforward(dim,heads,max_len)\n",
        "    self.norm2=LayerNorm(dim)\n",
        "    self.drop = nn.Dropout(0.1)\n",
        "  def forward(self,x,mask):\n",
        "    out=self.attention(x,mask)\n",
        "    out=x+out\n",
        "    out=self.norm1(x)\n",
        "    f=out\n",
        "    out=self.ff(out)\n",
        "    out=self.norm2(out+f)\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oGUfCZnQQxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AllEncode(nn.Module):\n",
        "  def __init__(self,dim,heads,max_len,n_segs):\n",
        "    super().__init__()\n",
        "    self.embed=embedding(dim,len(tokenizer1.vocab),max_len,n_segs)\n",
        "    self.encoder1=Encoder(dim,heads,max_len)\n",
        "    self.encoder2=Encoder(dim,heads,max_len)\n",
        "    self.encoder3=Encoder(dim,heads,max_len)\n",
        "    self.encoder4=Encoder(dim,heads,max_len)\n",
        "    self.encoder5=Encoder(dim,heads,max_len)\n",
        "    self.encoder6=Encoder(dim,heads,max_len)\n",
        "\n",
        "  def forward(self,x,mask,seg):\n",
        "    out=self.embed(x,seg)\n",
        "    out=self.encoder1(out,mask)\n",
        "    out=self.encoder2(out,mask)\n",
        "    out=self.encoder3(out,mask)\n",
        "    out=self.encoder4(out,mask)\n",
        "    out=self.encoder5(out,mask)\n",
        "    out=self.encoder6(out,mask)\n",
        "\n",
        "    return out\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSNuo5owidQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertPreTrain(nn.Module):\n",
        "  def __init__(self,dim,heads,max_len,n_seg):\n",
        "    super().__init__()\n",
        "    self.allenc=AllEncode(dim,heads,max_len,n_seg)\n",
        "    self.fc1=nn.Linear(dim,dim)\n",
        "    self.tanh=nn.Tanh()\n",
        "    self.fc2=nn.Linear(dim,2)\n",
        "    self.norm=LayerNorm(dim)\n",
        "    embed_weight = self.allenc.embed.embed.weight\n",
        "    n_vocab, n_dim = embed_weight.size()\n",
        "    self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "    self.decoder.weight = embed_weight\n",
        "    self.linear = nn.Linear(dim,dim)\n",
        "\n",
        "  def forward(self,batch):\n",
        "    input_ids, segment_ids, input_mask, masked_ids, masked_pos, masked_weights, is_next=batch\n",
        "\n",
        "    out=self.allenc(input_ids,input_mask,segment_ids)\n",
        "\n",
        "    out1=self.fc1(out[:,0])\n",
        "    out1=self.tanh(out1)\n",
        "    out1=self.fc2(out1)\n",
        "\n",
        "    masked_pos1 = masked_pos[:, :, None].expand(-1, -1, out.size(-1))\n",
        "    h_masked = torch.gather(out, 1, masked_pos1)\n",
        "    h_masked = self.norm(gelu(self.linear(h_masked)))\n",
        "    out2 = self.decoder(h_masked)\n",
        "\n",
        "    return out1,out2\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62k_PM9B5LzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=BertPreTrain(768,12,512,2).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pww1hk7u5U3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion1=nn.CrossEntropyLoss().to(device)\n",
        "criterion2=nn.CrossEntropyLoss().to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYR-5sg36Ene",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "374b0571-ca6b-43c3-a98f-c018b9711f0c"
      },
      "source": [
        "!pip install -U pytorch_warmup"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_warmup\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/22/2fb600a06a1d1b493d54ac8fa6c41e96870985992fc504104e0620bc2ea4/pytorch_warmup-0.0.4-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_warmup) (1.5.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (1.18.4)\n",
            "Installing collected packages: pytorch-warmup\n",
            "Successfully installed pytorch-warmup-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "354rqE_lDH8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytorch_warmup as warmup\n",
        "optimizer = torch.optim.AdamW(x.parameters(), lr=0.0001, betas=(0.9, 0.999), weight_decay=0.01) \n",
        "warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH1M_ELYDX14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_func(model,batch):\n",
        "  input_ids, segment_ids, input_mask, masked_ids, masked_pos, masked_weights, is_next = batch\n",
        "  clsf,mlm=model(batch)\n",
        "\n",
        "  lossclf=criterion1(clsf,is_next)\n",
        "\n",
        "  losslm=criterion2(mlm.transpose(1,2),masked_ids)\n",
        "\n",
        "  return lossclf+losslm\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEfCbmv_JDh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=3\n",
        "step=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xx6bC8aJqSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  for i,batch in enumerate(data_loader.__iter__()):\n",
        "    batch = [t.to(device) for t in batch]\n",
        "    optimizer.zero_grad()\n",
        "    loss=loss_func(x,batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    if step <10 :\n",
        "      warmup_scheduler.dampen()\n",
        "    step=step+1\n",
        "\n",
        "    print(\"LOSS:\",loss,\" \",\"epoch[%d/%d]\"%(epoch,epochs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuXAxESpKBbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}